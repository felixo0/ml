{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "          \"capital_gain\", \"capital_loss\", \"hour_per_week\", \"native_country\",\n",
    "          \"income_bracket\"]\n",
    "\n",
    "EL_COLUMN = \"label\"\n",
    "EGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "                   \"relationship\", \"race\",\"gender\",\"native_country\"]\n",
    "TINUOUS_COLUMNS = [\"age\", \"education_num\", \"captital_gain\", \"capital_loss\",\n",
    "                  \"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(train_data, test_data):\n",
    "    \"\"\"Maybe downloads trainging data and return train test file names.\"\"\"\n",
    "    if train_data:\n",
    "        train_file_name = train_data\n",
    "    else:\n",
    "        train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrive(\"http:mlr.cs.umss.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)\n",
    "        train_file_name = train_file.name\n",
    "        train_file.close()\n",
    "        print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "        \n",
    "    if test_data:\n",
    "        test_file_data = test_data\n",
    "    else:\n",
    "        test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrive(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)\n",
    "        test_file_name = test_file.name\n",
    "        test_file.close()\n",
    "        print(\"Test data is downloaded to %s %test_file_name\")\n",
    "    return train_file_name, test_file_name\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "    \"\"\"Build an estimator\"\"\"\n",
    "    \n",
    "    # Sparse base columns.\n",
    "    gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\",\n",
    "                                                       keys=[\"female\", \"male\"])\n",
    "    education = tf.contrib.layers.sparce_column_with_hassh_bucket(\n",
    "        \"education\", hash_bucket_size=1000)\n",
    "    relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "        \"relationship\", hash_bukcet_size=100)\n",
    "    workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "        \"workclass\", hash_bukcet_size=100)\n",
    "    occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "        \"occupation\", hash_bukcet_size=1000)\n",
    "    native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "        \"native_country\", hash_bukcet_size=1000)\n",
    "    \n",
    "    # Continuous base columns.\n",
    "    age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "    education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "    capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "    capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "    hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n",
    "    \n",
    "    # Transformation\n",
    "    age_buckets = tf.contrib.layers.bucketized_column(age,\n",
    "                                                     boundaries=[\n",
    "                                                         18, 25, 30, 35, 40, 45,\n",
    "                                                         50, 55, 60 ,65\n",
    "                                                     ])\n",
    "    \n",
    "    wide_columns = [gender, native_country, education, occupation, workclass,\n",
    "                   relationship, age_buckets,\n",
    "                   tf.contrib.layers.cross_column([education, occupation],\n",
    "                                                 hash_bucket_size=int(1e4)),\n",
    "                   tf.contrib.layers.crossed_column([age_buckets, education, occupation],\n",
    "                                                    hash_bucket_size=int(1e6)),\n",
    "                   tf.contrib.layers.crossed_column([native_country, occupation],\n",
    "                                                    hash_bucket_size=int(1e4))\n",
    "                   ]\n",
    "    \n",
    "    deep_columns = [\n",
    "        tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "        tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "        tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "        tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "        tf.contrib.layers.embedding_column(native_country,\n",
    "                                         dimension=8),\n",
    "        tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "        age,\n",
    "        education_num,\n",
    "        capital_gain,\n",
    "        capital_loss,\n",
    "        hours_per_week\n",
    "    ]\n",
    "    \n",
    "    if model_type == \"wide\":\n",
    "        m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n",
    "                                             feature_columns=wide_columns)\n",
    "    elif model_type == \"deep\":\n",
    "        m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n",
    "                                           feature_columns=deep_columns,\n",
    "                                           hidden_units=[100,50])\n",
    "    else:\n",
    "        m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "            model_dir=model_dir,\n",
    "            linear_feature_columns=wide_columns,\n",
    "            dnn_feature_columns=deep_columns,\n",
    "            dnn_hidden_units=[100,50])\n",
    "    \n",
    "    return  m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {\n",
    "      k: tf.SparseTensor(\n",
    "          indices=[[i, 0] for i in range(df[k].size)],\n",
    "          values=df[k].values,\n",
    "          dense_shape=[df[k].size, 1])\n",
    "      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download(train_data, test_data)\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "\n",
    "  # remove NaN elements\n",
    "  df_train = df_train.dropna(how='any', axis=0)\n",
    "  df_test = df_test.dropna(how='any', axis=0)\n",
    "\n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "  model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir, model_type)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "                 FLAGS.train_data, FLAGS.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  parser.add_argument(\n",
    "      \"--model_dir\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Base directory for output models.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--model_type\",\n",
    "      type=str,\n",
    "      default=\"wide_n_deep\",\n",
    "      help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--train_steps\",\n",
    "      type=int,\n",
    "      default=200,\n",
    "      help=\"Number of training steps.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--train_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to the training data.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--test_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to the test data.\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
